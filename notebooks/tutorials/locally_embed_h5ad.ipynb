{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAscVI Tutorial: Loading a Checkpoint and Predicting on an h5ad File\n",
    "\n",
    "This tutorial demonstrates how to use a pre-trained BAscVI model to generate embeddings for a single-cell RNA dataset in h5ad format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the BAscVI package\n",
    "!pip install -e ../..\n",
    "\n",
    "# Install additional dependencies (if needed)\n",
    "!pip install scanpy pytorch-lightning torch nbformat>=4.2.0\n",
    "\n",
    "# TODO: Update hugging face model: PLEASE UNCOMMENT THE LEGACY MODEL CHANGES TO THE ENCODER and BENCODER IN THE MODEL FILE FOR NOW (these contain the number 3019)\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "# Add the src directory to the path so we can import ml_benchmarking\n",
    "# Adjust this path if needed to point to the src directory containing ml_benchmarking\n",
    "sys.path.append('../../src')\n",
    "\n",
    "from ml_benchmarking.scripts.predict import predict\n",
    "\n",
    "# Optional imports for visualization\n",
    "import umap\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint already exists at checkpoints/human_bascvi_epoch_123.ckpt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Download the pre-trained BAscVI human model\n",
    "checkpoint_url = \"https://huggingface.co/phenomicai/bascvi-human/resolve/main/human_bascvi_epoch_123.ckpt\"\n",
    "checkpoint_path = \"checkpoints/human_bascvi_epoch_123.ckpt\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"Downloading checkpoint from {checkpoint_url}...\")\n",
    "    response = requests.get(checkpoint_url)\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Checkpoint already exists at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Your h5ad File\n",
    "\n",
    "Replace this path with your own h5ad file. All h5ads in the same directory will be used, so if you only want to use one, make sure it's the only one in the directory.\n",
    "\n",
    "IMPORTANT: The BAscVI model expects the gene names to be in the 'gene' column of the var dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 65662 × 24835\n",
      "    obs: 'nGene', 'nUMI', 'channel', 'region', 'percent.ribo', 'free_annotation', 'donor_id', 'sample', 'location', 'magnetic.selection', 'preparation.site', 'compartment', 'tissue_ontology_term_id', 'assay_ontology_term_id', 'disease_ontology_term_id', 'development_stage_ontology_term_id', 'cell_type_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'sex_ontology_term_id', 'is_primary_data', 'organism_ontology_term_id', 'suspension_type', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
      "    var: 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'gene'\n",
      "    uns: 'cell_type_ontology_term_id_colors', 'citation', 'default_embedding', 'donor_id_colors', 'schema_reference', 'schema_version', 'title'\n",
      "    obsm: 'X_Compartment_tSNE', 'X_tSNE'\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_H5AD = None\n",
    "\n",
    "# Create a directory for the input data\n",
    "os.makedirs(\"input_data\", exist_ok=True)\n",
    "\n",
    "if not PATH_TO_H5AD:\n",
    "    if not os.path.exists(os.path.join(\"input_data\",\"0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad\")):\n",
    "        # wget example scRNA dataset from cellxgene\n",
    "        !wget -P {\"input_data\"} https://datasets.cellxgene.cziscience.com/0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad \n",
    "\n",
    "    # Run inference\n",
    "    PATH_TO_H5AD = os.path.join(\"input_data\",\"0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad\")  # h5ad path is the path to the single cell RNA .h5ad file intended to be uploaded and embeded\n",
    "    adata = sc.read_h5ad(PATH_TO_H5AD)\n",
    "\n",
    "    # Add the gene names to the gene column\n",
    "    adata.var[\"gene\"] = adata.var[\"feature_name\"]\n",
    "\n",
    "    # Save the adata object\n",
    "    adata.write_h5ad(PATH_TO_H5AD)\n",
    "\n",
    "\n",
    "else:\n",
    "    adata = sc.read_h5ad(PATH_TO_H5AD)\n",
    "\n",
    "# load the data, check that the gene names are in the 'gene' column of the var dataframe\n",
    "assert \"gene\" in adata.var.columns, \"The gene names must be in the 'gene' column of the var dataframe.\"\n",
    "\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure sufficient overlap between model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint and print gene_list from the checkpoint hyperparameters\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract the gene list from the hyperparameters\n",
    "gene_list = checkpoint['hyper_parameters']['gene_list']\n",
    "print(f\"Loaded checkpoint with {len(gene_list)} genes\")\n",
    "#print(\"model gene_list: \", gene_list)\n",
    "#print(\"adata.var.gene: \", adata.var.gene.values.tolist())\n",
    "\n",
    "# Check if our data has the same genes\n",
    "common_genes = set(gene_list).intersection(set(adata.var.gene))\n",
    "percent_overlap = len(common_genes)/len(gene_list)*100\n",
    "print(f\"Number of genes in common between model and data: {len(common_genes)} out of {len(gene_list)} in model. ({percent_overlap}%)\")\n",
    "\n",
    "assert len(common_genes) > 0, \"No genes in common between model and data. Please check the gene names in the model and the data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Configuration Dictionary\n",
    "\n",
    "We'll define a configuration dictionary that the predict function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"prediction_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# checkpoint_path = \"/home/ubuntu/paper_repo/bascvi/runs/scref_train/scref_train/xgo52kj8/checkpoints/scvi-vae-epoch=31-elbo_val=0.00.ckpt\"\n",
    "\n",
    "# Define configuration\n",
    "config = {\n",
    "    \"mode\": \"predict\",\n",
    "    \"pretrained_model_path\": checkpoint_path,\n",
    "    \"run_save_dir\": output_dir,\n",
    "    \"embedding_file_name\": \"bascvi_embeddings_\" + os.path.basename(checkpoint_path),\n",
    "    \n",
    "    \"datamodule\": {\n",
    "        \"class_name\": \"AnnDataDataModule\",\n",
    "        \"options\": {\n",
    "            \"data_root_dir\": os.path.dirname(PATH_TO_H5AD),  # Directory containing h5ad files\n",
    "            \"dataloader_args\": {\n",
    "                \"batch_size\": 64,\n",
    "                \"num_workers\": 0,\n",
    "                \"pin_memory\": True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"trainer_module_name\": \"bascvi_trainer\",\n",
    "    \"trainer_class_name\": \"BAScVITrainer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Prediction Using the BAscVI Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "--------------Loading model from checkpoint-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction process...\n",
      "Not using macrogene matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['vae.z_predictor.predictor.0.weight', 'vae.z_predictor.predictor.0.bias', 'vae.z_predictor.predictor.1.weight', 'vae.z_predictor.predictor.1.bias', 'vae.z_predictor.predictor.1.running_mean', 'vae.z_predictor.predictor.1.running_var', 'vae.z_predictor.predictor.1.num_batches_tracked', 'vae.z_predictor.predictor.3.weight', 'vae.z_predictor.predictor.3.bias', 'vae.z_predictor.predictor.4.weight', 'vae.z_predictor.predictor.4.bias', 'vae.z_predictor.predictor.4.running_mean', 'vae.z_predictor.predictor.4.running_var', 'vae.z_predictor.predictor.4.num_batches_tracked', 'vae.x_predictor.predictor.0.weight', 'vae.x_predictor.predictor.0.bias', 'vae.x_predictor.predictor.1.weight', 'vae.x_predictor.predictor.1.bias', 'vae.x_predictor.predictor.1.running_mean', 'vae.x_predictor.predictor.1.running_var', 'vae.x_predictor.predictor.1.num_batches_tracked', 'vae.x_predictor.predictor.3.weight', 'vae.x_predictor.predictor.3.bias', 'vae.x_predictor.predictor.4.weight', 'vae.x_predictor.predictor.4.bias', 'vae.x_predictor.predictor.4.running_mean', 'vae.x_predictor.predictor.4.running_var', 'vae.x_predictor.predictor.4.num_batches_tracked']\n",
      "--------------Setting up data module--------------\n",
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/anndata/_core/anndata.py:402: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "--------------Embedding prediction on full dataset-------------\n",
      "model in eval mode\n",
      "predict dataloader created\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage = Predicting on AnnDatas\n",
      "# of files:  1\n",
      "Pretrained batch size:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:122: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: |          | 0/? [00:00<?, ?it/s]# genes found in reference:  20091 # genes in adata 24835 # genes in reference 29494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/anndata/_core/merge.py:1358: UserWarning: Only some AnnData objects have `.raw` attribute, not concatenating `.raw` attributes.\n",
      "  warn(\n",
      "/home/ubuntu/paper_repo/bascvi/.venv/lib/python3.9/site-packages/anndata/_core/anndata.py:747: UserWarning: \n",
      "AnnData expects .obs.index to contain strings, but got values like:\n",
      "    [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "\n",
      "    Inferred to be: floating\n",
      "\n",
      "  value_idx = self._prep_dim_index(value.index, attr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1026/1026 [01:55<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predictions made\n",
      "-----------------------Save Embeddings------------------------\n",
      "Saved predicted embeddings to: prediction_results/bascvi_embeddings_human_bascvi_epoch_123.ckpt.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete! Results saved to prediction_results\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed_everything(42)\n",
    "\n",
    "# Run prediction\n",
    "print(\"Starting prediction process...\")\n",
    "predict(config)\n",
    "print(f\"Prediction complete! Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Visualize Embeddings\n",
    "\n",
    "Let's load the generated embeddings and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating UMAP visualization...\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings\n",
    "embedding_file = os.path.join(output_dir, \"bascvi_embeddings_\" + os.path.basename(checkpoint_path) + \".tsv\")\n",
    "embeddings_df = pd.read_csv(embedding_file, sep=\"\\t\")\n",
    "\n",
    "# Extract the embedding columns\n",
    "emb_columns = [col for col in embeddings_df.columns if col.startswith(\"embedding_\")]\n",
    "\n",
    "# Create a UMAP visualization\n",
    "print(\"Generating UMAP visualization...\")\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,  # Lower for more local structure\n",
    "    min_dist=0.5,    # Higher value to spread points out more\n",
    "    metric='euclidean',\n",
    "    random_state=42,\n",
    "    n_jobs=1         # Parallel processing\n",
    ")\n",
    "umap_embeddings = reducer.fit_transform(embeddings_df[emb_columns].values)\n",
    "\n",
    "# Add UMAP coordinates to the dataframe\n",
    "embeddings_df[\"UMAP1\"] = umap_embeddings[:, 0]\n",
    "embeddings_df[\"UMAP2\"] = umap_embeddings[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the obs column you'd like to color by to the dataframe\n",
    "OBS_COL = \"cell_type\"\n",
    "\n",
    "adata.obs[\"cell_counter\"] = range(adata.obs.shape[0])\n",
    "\n",
    "embeddings_df = embeddings_df.merge(adata.obs[[\"cell_counter\", OBS_COL]], on=\"cell_counter\", how=\"left\")\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.scatter(\n",
    "    embeddings_df, x=\"UMAP1\", y=\"UMAP2\", \n",
    "    color=\"OBS_COL\",\n",
    "    title=\"UMAP Visualization of BAscVI Embeddings\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Save the visualization\n",
    "umap_html_path = os.path.join(output_dir, \"umap_visualization.html\")\n",
    "fig.write_html(umap_html_path)\n",
    "print(f\"UMAP visualization saved to {umap_html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add Embeddings to Original AnnData for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the original AnnData object\n",
    "adata = sc.read_h5ad(PATH_TO_H5AD)\n",
    "\n",
    "# Match cell indices with embeddings\n",
    "if \"cell_counter\" in embeddings_df.columns:\n",
    "    # Add embeddings to AnnData obsm\n",
    "    cell_indices = embeddings_df[\"cell_counter\"].values.astype(int)\n",
    "    adata.obsm[\"X_bascvi\"] = np.zeros((adata.n_obs, len(emb_columns)))\n",
    "    \n",
    "    # Only assign embeddings to cells that are in the embeddings dataframe\n",
    "    valid_indices = cell_indices[cell_indices < adata.n_obs]\n",
    "    adata.obsm[\"X_bascvi\"][valid_indices] = embeddings_df[emb_columns].values[cell_indices < adata.n_obs]\n",
    "    \n",
    "    # Compute neighborhood graph and UMAP using BAscVI embeddings\n",
    "    sc.pp.neighbors(adata, use_rep=\"X_bascvi\")\n",
    "    sc.tl.umap(adata)\n",
    "    \n",
    "    # Optional: run clustering\n",
    "    sc.tl.leiden(adata)\n",
    "    \n",
    "    # Visualize\n",
    "    sc.pl.umap(adata, color=[\"leiden\"])\n",
    "    \n",
    "    # Save annotated AnnData\n",
    "    adata.write(os.path.join(output_dir, \"adata_with_bascvi_embeddings.h5ad\"))\n",
    "    print(f\"AnnData with BAscVI embeddings saved to {os.path.join(output_dir, 'adata_with_bascvi_embeddings.h5ad')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've learned how to:\n",
    "1. Download a pre-trained BAscVI model\n",
    "2. Configure and run the prediction pipeline on an h5ad file\n",
    "3. Visualize the resulting embeddings using UMAP\n",
    "4. Integrate the embeddings back into your AnnData object for downstream analysis\n",
    "\n",
    "These embeddings can be used for various downstream tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
