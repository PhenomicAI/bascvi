{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf53eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/train'\n",
    "TEMP_DIR = './reference_data/temp_store/'\n",
    "file_paths = glob.glob(os.path.join(DATA_DIR, \"*.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67903703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile: \n",
    "#   1) List of all genes and frequency in files\n",
    "#   2) List of all barcodes\n",
    "\n",
    "gene_dict = {}\n",
    "all_barcodes = []\n",
    "\n",
    "for f in file_paths:\n",
    "    adata_backed = scanpy.read(f,backed='r')\n",
    "    gene_list_ = adata_backed.var['gene'].values\n",
    "    for g in gene_list_:\n",
    "        if g in gene_dict:\n",
    "            gene_dict[g] += 1\n",
    "        else:\n",
    "            gene_dict[g] = 1\n",
    "\n",
    "    all_barcodes += list(adata_backed.obs.index.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.asarray(list(gene_dict.values()))\n",
    "plt.hist(freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0795a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "DATA_DIR = './data/train'\n",
    "TEMP_DIR = './reference_data/temp_store/'\n",
    "file_paths = glob.glob(os.path.join(DATA_DIR, \"*.h5ad\"))\n",
    "\n",
    "# Compile: \n",
    "#   1) List of all genes and frequency in files\n",
    "#   2) List of all barcodes\n",
    "\n",
    "gene_dict = {}\n",
    "all_barcodes = []\n",
    "\n",
    "for f in file_paths:\n",
    "    adata_backed = scanpy.read(f,backed='r')\n",
    "    gene_list_ = adata_backed.var['gene'].values\n",
    "    for g in gene_list_:\n",
    "        if g in gene_dict:\n",
    "            gene_dict[g] += 1\n",
    "        else:\n",
    "            gene_dict[g] = 1\n",
    "\n",
    "    all_barcodes += list(adata_backed.obs.index.values)\n",
    "\n",
    "# Specify reference gene list\n",
    "\n",
    "reference_gene_list = [k for k,v in gene_dict.items() if v>5]\n",
    "\n",
    "# Save reference gene list\n",
    "\n",
    "with open(TEMP_DIR + 'reference_genes.list', 'wb') as handle:\n",
    "    pkl.dump(reference_gene_list, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def log_mean(g, X):\n",
    "    \n",
    "    vals = X[g.values,:]\n",
    "    log_counts = np.log(vals.sum(axis=1))\n",
    "    local_mean = np.mean(log_counts).astype(np.float32)\n",
    "    return local_mean\n",
    "\n",
    "def log_var(g, X):\n",
    "    \n",
    "    vals = X[g.values,:]\n",
    "    log_counts = np.log(vals.sum(axis=1))\n",
    "    local_var = np.var(log_counts).astype(np.float32)\n",
    "    return local_var\n",
    "\n",
    "batch_keys = ['sample_name','study_name']\n",
    "\n",
    "b_size = 20000 # Block size parameter for random chunking of adatas\n",
    "n_cells = len(all_barcodes)\n",
    "block_n = n_cells//b_size\n",
    "\n",
    "random_inds = np.random.permutation(np.arange(n_cells))\n",
    "block_mapping = {all_barcodes[random_inds[i]]:i//b_size for i in range(n_cells)}\n",
    "\n",
    "print('Splitting data into :',block_n, ' blocks')\n",
    "# Create temp folder for saving adata blocks\n",
    "\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.makedirs(TEMP_DIR)\n",
    "\n",
    "# Chunk adatas into blocks - load in n adata (nad) at once to save on file write number\n",
    "\n",
    "nad = 3\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "batch_counters = {b:0 for b in batch_keys}\n",
    "split_paths = []\n",
    "batch_dict = {}\n",
    "\n",
    "for ii in range(0,len(file_paths),nad):\n",
    "    adatas = []\n",
    "    \n",
    "    for jj in range(nad):\n",
    "        if ii+jj < len(file_paths):\n",
    "            \n",
    "            f_path = file_paths[ii+jj]\n",
    "            \n",
    "            print('Reading File: ', ii+jj, ' out of ', len(file_paths), ' file name is ', f_path)            \n",
    "            adata_ = scanpy.read(f_path)\n",
    "            \n",
    "            if not adata_.X.dtype == np.float32:\n",
    "                adata_.X = adata_.X.astype(np.float32)\n",
    "\n",
    "            ref = scanpy.AnnData(X=np.zeros((1,len(reference_gene_list)),dtype=np.float32),var={'gene':reference_gene_list})\n",
    "            ref.var = ref.var.set_index(ref.var['gene'])\n",
    "            adata_ = scanpy.concat([ref,adata_],join='outer')\n",
    "            adata_ = adata_[:,reference_gene_list]\n",
    "            \n",
    "            #Filter cells with very few gene reads\n",
    "            \n",
    "            gene_counts = adata_.X.getnnz(axis=1)\n",
    "            mask = gene_counts > 300\n",
    "            adata_ = adata_[mask,:].copy()\n",
    "            \n",
    "            #Batch calcs - assign batch ID that's unique across all datasets\n",
    "            \n",
    "            for i,b in enumerate(batch_keys):\n",
    "                batch_id = \"batch_\" + str(i+1)\n",
    "                codes = adata_.obs[b].astype(\"category\").cat.codes.astype(int)\n",
    "                adata_.obs[batch_id] = codes + batch_counters[b]\n",
    "                batch_counters[b] += codes.max() + 1\n",
    "                batch_dict[batch_id] = codes.max() + 1\n",
    "                \n",
    "            #Add local_l_mean_key and local_l_var_key to adata.obs\n",
    "            \n",
    "            adata_.obs['int_index'] = list(range(adata_.shape[0]))\n",
    "            \n",
    "            for i in range(len(batch_keys)):\n",
    "\n",
    "                header_m = \"l_mean_batch_\" + str(i+1)\n",
    "                adata_.obs[header_m] = adata_.obs.groupby(batch_keys[i])[\"int_index\"].transform(log_mean, adata_.X)\n",
    "                header_v = \"l_var_batch_\" + str(i+1)\n",
    "                adata_.obs[header_v] = adata_.obs.groupby(batch_keys[i])[\"int_index\"].transform(log_var, adata_.X)\n",
    "                    \n",
    "            adatas.append(adata_)\n",
    "            \n",
    "    adata_chunk = scanpy.concat(adatas, join=\"inner\", index_unique=None)\n",
    "    adata_chunk.obs['block'] = adata_chunk.obs['barcode'].apply(lambda x : block_mapping[x])\n",
    "\n",
    "    print('Writing split : ', ii//nad)\n",
    "    for i in range(block_n):\n",
    "        \n",
    "        split_path = TEMP_DIR + 'chunk_' + str(ii//nad) + '_split_' + str(i) + '.h5ad'\n",
    "        \n",
    "        adata_split = adata_chunk[adata_chunk.obs['block']==i,:].copy()\n",
    "        adata_split.write(split_path)\n",
    "        split_paths.append(split_path)\n",
    "\n",
    "with open(TEMP_DIR+'bdict.dict', \"wb\") as fh:\n",
    "    pkl.dump([batch_dict, batch_keys], fh)\n",
    "    \n",
    "print('Consolidating split blocks...')\n",
    "\n",
    "for i in range(block_n):\n",
    "    adatas = []\n",
    "    for split_path in split_paths: \n",
    "        if split_path[-6] == str(i):\n",
    "            adata_ = scanpy.read(split_path)\n",
    "            adatas.append(adata_)\n",
    "            os.remove(split_path)\n",
    "            \n",
    "    adata_block = scanpy.concat(adatas, join=\"inner\", index_unique=None)\n",
    "    adata_block.write(TEMP_DIR+'adata_block_'+str(i)+'.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/train'\n",
    "TEMP_DIR = './reference_data/temp_store/'\n",
    "file_paths = glob.glob(os.path.join(TEMP_DIR, \"*.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef37af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20629b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "obs = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    obs.append(scanpy.read(file_paths[i],backed='r').obs)\n",
    "\n",
    "obs = pd.concat(obs)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efe93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
